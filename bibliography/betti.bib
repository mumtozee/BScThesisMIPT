@inproceedings{genre,
    title = "Extracting and Inferring Personal Attributes from Dialogue",
    author = "Wang, Zhilin  and
      Zhou, Xuhui  and
      Koncel-Kedziorski, Rik  and
      Marin, Alex  and
      Xia, Fei",
    booktitle = "Proceedings of the 4th Workshop on NLP for Conversational AI",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.nlp4convai-1.6",
    doi = "10.18653/v1/2022.nlp4convai-1.6",
    pages = "58--69",
    abstract = "Personal attributes represent structured information about a person, such as their hobbies, pets, family, likes and dislikes. We introduce the tasks of extracting and inferring personal attributes from human-human dialogue, and analyze the linguistic demands of these tasks. To meet these challenges, we introduce a simple and extensible model that combines an autoregressive language model utilizing constrained attribute generation with a discriminative reranker. Our model outperforms strong baselines on extracting personal attributes as well as inferring personal attributes that are not contained verbatim in utterances and instead requires commonsense reasoning and lexical inferences, which occur frequently in everyday conversation. Finally, we demonstrate the benefit of incorporating personal attributes in social chit-chat and task-oriented dialogue settings.",
}

@inproceedings{gtky,
    title = "Getting To Know You: User Attribute Extraction from Dialogues",
    author = "Wu, Chien-Sheng  and
      Madotto, Andrea  and
      Lin, Zhaojiang  and
      Xu, Peng  and
      Fung, Pascale",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.73",
    pages = "581--589",
    abstract = "User attributes provide rich and useful information for user understanding, yet structured and easy-to-use attributes are often sparsely populated. In this paper, we leverage dialogues with conversational agents, which contain strong suggestions of user information, to automatically extract user attributes. Since no existing dataset is available for this purpose, we apply distant supervision to train our proposed two-stage attribute extractor, which surpasses several retrieval and generation baselines on human evaluation. Meanwhile, we discuss potential applications (e.g., personalized recommendation and dialogue systems) of such extracted user attributes, and point out current limitations to cast light on future work.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{vaswani2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{devlin_bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{deberta,
  author       = {Pengcheng He and
                  Xiaodong Liu and
                  Jianfeng Gao and
                  Weizhu Chen},
  title        = {DeBERTa: Decoding-enhanced {BERT} with Disentangled Attention},
  journal      = {CoRR},
  volume       = {abs/2006.03654},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.03654},
  eprinttype    = {arXiv},
  eprint       = {2006.03654},
  timestamp    = {Mon, 30 May 2022 13:48:56 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-03654.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{personachat,
  author       = {Saizheng Zhang and
                  Emily Dinan and
                  Jack Urbanek and
                  Arthur Szlam and
                  Douwe Kiela and
                  Jason Weston},
  title        = {Personalizing Dialogue Agents: {I} have a dog, do you have pets too?},
  journal      = {CoRR},
  volume       = {abs/1801.07243},
  year         = {2018},
  url          = {http://arxiv.org/abs/1801.07243},
  eprinttype    = {arXiv},
  eprint       = {1801.07243},
  timestamp    = {Mon, 13 Aug 2018 16:46:43 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1801-07243.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dnli,
    title = "Dialogue Natural Language Inference",
    author = "Welleck, Sean  and
      Weston, Jason  and
      Szlam, Arthur  and
      Cho, Kyunghyun",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1363",
    doi = "10.18653/v1/P19-1363",
    pages = "3731--3741",
    abstract = "Consistency is a long standing issue faced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model{'}s consistency.",
}

@article{nli_survey,
  author       = {Shane Storks and
                  Qiaozi Gao and
                  Joyce Y. Chai},
  title        = {Commonsense Reasoning for Natural Language Understanding: {A} Survey
                  of Benchmarks, Resources, and Approaches},
  journal      = {CoRR},
  volume       = {abs/1904.01172},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.01172},
  eprinttype    = {arXiv},
  eprint       = {1904.01172},
  timestamp    = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-01172.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498",
    abstract = "The recent {``}Text-to-Text Transfer Transformer{''} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {``}accidental translation{''} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
}

@article{distilbert,
  author       = {Victor Sanh and
                  Lysandre Debut and
                  Julien Chaumond and
                  Thomas Wolf},
  title        = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper
                  and lighter},
  journal      = {CoRR},
  volume       = {abs/1910.01108},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.01108},
  eprinttype    = {arXiv},
  eprint       = {1910.01108},
  timestamp    = {Tue, 02 Jun 2020 12:48:59 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{focus,
  author       = {Yoonna Jang and
                  Jungwoo Lim and
                  Yuna Hur and
                  Dongsuk Oh and
                  Suhyune Son and
                  Yeonsoo Lee and
                  Dong{-}Hoon Shin and
                  Seungryong Kim and
                  Heuiseok Lim},
  title        = {Call for Customized Conversation: Customized Conversation Grounding
                  Persona and Knowledge},
  journal      = {CoRR},
  volume       = {abs/2112.08619},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.08619},
  eprinttype    = {arXiv},
  eprint       = {2112.08619},
  timestamp    = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-08619.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{pappu-rudnicky-2014-knowledge,
    title = "Knowledge Acquisition Strategies for Goal-Oriented Dialog Systems",
    author = "Pappu, Aasish  and
      Rudnicky, Alexander",
    booktitle = "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL})",
    month = jun,
    year = "2014",
    address = "Philadelphia, PA, U.S.A.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-4326",
    doi = "10.3115/v1/W14-4326",
    pages = "194--198",
}

@article{tigunova_2019,
  author       = {Anna Tigunova and
                  Andrew Yates and
                  Paramita Mirza and
                  Gerhard Weikum},
  title        = {Listening between the Lines: Learning Personal Attributes from Conversations},
  journal      = {CoRR},
  volume       = {abs/1904.10887},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.10887},
  eprinttype    = {arXiv},
  eprint       = {1904.10887},
  timestamp    = {Thu, 02 May 2019 15:13:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-10887.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mazare-etal-2018-training,
    title = "Training Millions of Personalized Dialogue Agents",
    author = "Mazar{\'e}, Pierre-Emmanuel  and
      Humeau, Samuel  and
      Raison, Martin  and
      Bordes, Antoine",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1298",
    doi = "10.18653/v1/D18-1298",
    pages = "2775--2779",
    abstract = "Current dialogue systems fail at being engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the dataset used in Zhang et al. (2018) is synthetic and only contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from Zhang et al. (2018) and achieving state-of-the-art results.",
}

@inproceedings{bosselut-etal-2019-comet,
    title = "{COMET}: Commonsense Transformers for Automatic Knowledge Graph Construction",
    author = "Bosselut, Antoine  and
      Rashkin, Hannah  and
      Sap, Maarten  and
      Malaviya, Chaitanya  and
      Celikyilmaz, Asli  and
      Choi, Yejin",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1470",
    doi = "10.18653/v1/P19-1470",
    pages = "4762--4779",
    abstract = "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5{\%} (ATOMIC) and 91.7{\%} (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",
}

@inproceedings{
alt2019improving,
title={Improving Relation Extraction by Pre-trained Language Representations},
author={Christoph Alt and Marc H{\"u}bner and Leonhard Hennig},
booktitle={Automated Knowledge Base Construction (AKBC)},
year={2019},
url={https://openreview.net/forum?id=BJgrxbqp67}
}

@inproceedings{banko2007,
author = {Banko, Michele and Cafarella, Michael J. and Soderland, Stephen and Broadhead, Matt and Etzioni, Oren},
title = {Open Information Extraction from the Web},
year = {2007},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Traditionally, Information Extraction (IE) has focused on satisfying precise, narrow, pre-specified requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations.This paper introduces Open IE (OIE), a new extraction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces TEXTRUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efficient extraction and exploration via user queries.We report on experiments over a 9,000,000 Web page corpus that compare TEXTRUNNER with KNOWITALL, a state-of-the-art Web IE system. TEXTRUNNER achieves an error reduction of 33\% on a comparable set of extractions. Furthermore, in the amount of time it takes KNOWITALL to perform extraction for a handful of pre-specified relations, TEXTRUNNER extracts a far broader set of facts reflecting orders of magnitude more relations, discovered on the fly. We report statistics on TEXTRUNNER's 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000 more abstract assertions.},
booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
pages = {2670–2676},
numpages = {7},
location = {Hyderabad, India},
series = {IJCAI'07}
}

@inproceedings{wu-weld-2010-open,
    title = "Open Information Extraction Using {W}ikipedia",
    author = "Wu, Fei  and
      Weld, Daniel S.",
    booktitle = "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P10-1013",
    pages = "118--127",
}

@inproceedings{berant-etal-2011-global,
    title = "Global Learning of Typed Entailment Rules",
    author = "Berant, Jonathan  and
      Dagan, Ido  and
      Goldberger, Jacob",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-1062",
    pages = "610--619",
}

@inproceedings{fader2014,
author = {Fader, Anthony and Zettlemoyer, Luke and Etzioni, Oren},
title = {Open Question Answering over Curated and Extracted Knowledge Bases},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623677},
doi = {10.1145/2623330.2623677},
abstract = {We consider the problem of open-domain question answering (Open QA) over massive knowledge bases (KBs). Existing approaches use either manually curated KBs like Freebase or KBs automatically extracted from unstructured text. In this paper, we present OQA, the first approach to leverage both curated and extracted KBs.A key technical challenge is designing systems that are robust to the high variability in both natural language questions and massive KBs. OQA achieves robustness by decomposing the full Open QA problem into smaller sub-problems including question paraphrasing and query reformulation. OQA solves these sub-problems by mining millions of rules from an unlabeled question corpus and across multiple KBs. OQA then learns to integrate these rules by performing discriminative training on question-answer pairs using a latent-variable structured perceptron algorithm. We evaluate OQA on three benchmark question sets and demonstrate that it achieves up to twice the precision and recall of a state-of-the-art Open QA system.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1156–1165},
numpages = {10},
keywords = {algorithms, experimentation},
location = {New York, New York, USA},
series = {KDD '14}
}

@inproceedings{mausam-etal-2012-open,
    title = "Open Language Learning for Information Extraction",
    author = "{Mausam}  and
      Schmitz, Michael  and
      Soderland, Stephen  and
      Bart, Robert  and
      Etzioni, Oren",
    booktitle = "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D12-1048",
    pages = "523--534",
}

@inproceedings{delcorro2013,
author = {Del Corro, Luciano and Gemulla, Rainer},
title = {ClausIE: Clause-Based Open Information Extraction},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488420},
doi = {10.1145/2488388.2488420},
abstract = {We propose ClausIE, a novel, clause-based approach to open information extraction, which extracts relations and their arguments from natural language text. ClausIE fundamentally differs from previous approaches in that it separates the detection of ``useful'' pieces of information expressed in a sentence from their representation in terms of extractions. In more detail, ClausIE exploits linguistic knowledge about the grammar of the English language to first detect clauses in an input sentence and to subsequently identify the type of each clause according to the grammatical function of its constituents. Based on this information, ClausIE is able to generate high-precision extractions; the representation of these extractions can be flexibly customized to the underlying application. ClausIE is based on dependency parsing and a small set of domain-independent lexica, operates sentence by sentence without any post-processing, and requires no training data (whether labeled or unlabeled). Our experimental study on various real-world datasets suggests that ClausIE obtains higher recall and higher precision than existing approaches, both on high-quality text as well as on noisy text as found in the web.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {355–366},
numpages = {12},
keywords = {open information extraction, relation extraction},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{zeng-etal-2014-relation,
    title = "Relation Classification via Convolutional Deep Neural Network",
    author = "Zeng, Daojian  and
      Liu, Kang  and
      Lai, Siwei  and
      Zhou, Guangyou  and
      Zhao, Jun",
    booktitle = "Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Dublin City University and Association for Computational Linguistics",
    url = "https://aclanthology.org/C14-1220",
    pages = "2335--2344",
}

@inproceedings{xu-etal-2015-classifying,
    title = "Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths",
    author = "Xu, Yan  and
      Mou, Lili  and
      Li, Ge  and
      Chen, Yunchuan  and
      Peng, Hao  and
      Jin, Zhi",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1206",
    doi = "10.18653/v1/D15-1206",
    pages = "1785--1794",
}

@inproceedings{stanovsky-etal-2018-supervised,
    title = "Supervised Open Information Extraction",
    author = "Stanovsky, Gabriel  and
      Michael, Julian  and
      Zettlemoyer, Luke  and
      Dagan, Ido",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1081",
    doi = "10.18653/v1/N18-1081",
    pages = "885--895",
    abstract = "We present data and methods that enable a supervised learning approach to Open Information Extraction (Open IE). Central to the approach is a novel formulation of Open IE as a sequence tagging problem, addressing challenges such as encoding multiple extractions for a predicate. We also develop a bi-LSTM transducer, extending recent deep Semantic Role Labeling models to extract Open IE tuples and provide confidence scores for tuning their precision-recall tradeoff. Furthermore, we show that the recently released Question-Answer Meaning Representation dataset can be automatically converted into an Open IE corpus which significantly increases the amount of available training data. Our supervised model outperforms the existing state-of-the-art Open IE systems on benchmark datasets.",
}

@inproceedings{vashishth-etal-2018-reside,
    title = "{RESIDE}: Improving Distantly-Supervised Neural Relation Extraction using Side Information",
    author = "Vashishth, Shikhar  and
      Joshi, Rishabh  and
      Prayaga, Sai Suman  and
      Bhattacharyya, Chiranjib  and
      Talukdar, Partha",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1157",
    doi = "10.18653/v1/D18-1157",
    pages = "1257--1266",
    abstract = "Distantly-supervised Relation Extraction (RE) methods train an extractor by automatically aligning relation instances in a Knowledge Base (KB) with unstructured text. In addition to relation instances, KBs often contain other relevant side information, such as aliases of relations (e.g., founded and co-founded are aliases for the relation founderOfCompany). RE models usually ignore such readily available side information. In this paper, we propose RESIDE, a distantly-supervised neural relation extraction method which utilizes additional side information from KBs for improved relation extraction. It uses entity type and relation alias information for imposing soft constraints while predicting relations. RESIDE employs Graph Convolution Networks (GCN) to encode syntactic information from text and improves performance even when limited side information is available. Through extensive experiments on benchmark datasets, we demonstrate RESIDE{'}s effectiveness. We have made RESIDE{'}s source code available to encourage reproducible research.",
}

@inproceedings{dstc_survey,
    title = "{``}Do you follow me?{''}: A Survey of Recent Approaches in Dialogue State Tracking",
    author = "Jacqmin, L{\'e}o  and
      Rojas Barahona, Lina M.  and
      Favre, Benoit",
    booktitle = "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = sep,
    year = "2022",
    address = "Edinburgh, UK",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sigdial-1.33",
    pages = "336--350",
    abstract = "While communicating with a user, a task-oriented dialogue system has to track the user{'}s needs at each turn according to the conversation history. This process called dialogue state tracking (DST) is crucial because it directly informs the downstream dialogue policy. DST has received a lot of interest in recent years with the text-to-text paradigm emerging as the favored approach. In this review paper, we first present the task and its associated datasets. Then, considering a large number of recent publications, we identify highlights and advances of research in 2021-2022. Although neural approaches have enabled significant progress, we argue that some critical aspects of dialogue systems such as generalizability are still underexplored. To motivate future studies, we propose several research avenues.",
}

@Article{binary_relevance,
author={Zhang, Min-Ling
and Li, Yu-Kun
and Liu, Xu-Ying
and Geng, Xin},
title={Binary relevance for multi-label learning: an overview},
journal={Frontiers of Computer Science},
year={2018},
month={Apr},
day={01},
volume={12},
number={2},
pages={191-202},
abstract={Multi-label learning deals with problems where each example is represented by a single instance while being associated with multiple class labels simultaneously. Binary relevance is arguably the most intuitive solution for learning from multi-label examples. It works by decomposing the multi-label learning task into a number of independent binary learning tasks (one per class label). In view of its potential weakness in ignoring correlations between labels, many correlation-enabling extensions to binary relevance have been proposed in the past decade. In this paper, we aim to review the state of the art of binary relevance from three perspectives. First, basic settings for multi-label learning and binary relevance solutions are briefly summarized. Second, representative strategies to provide binary relevancewith label correlation exploitation abilities are discussed. Third, some of our recent studies on binary relevance aimed at issues other than label correlation exploitation are introduced. As a conclusion, we provide suggestions on future research directions.},
issn={2095-2236},
doi={10.1007/s11704-017-7031-7},
url={https://doi.org/10.1007/s11704-017-7031-7}
}

@inproceedings{bengio_nlm,
 author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {A Neural Probabilistic Language Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf},
 volume = {13},
 year = {2000}
}

@inproceedings{dpr,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
}

@article{adamw,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Fixing Weight Decay Regularization in Adam},
  journal      = {CoRR},
  volume       = {abs/1711.05101},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.05101},
  eprinttype    = {arXiv},
  eprint       = {1711.05101},
  timestamp    = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{adafactor,
  author       = {Noam Shazeer and
                  Mitchell Stern},
  title        = {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  journal      = {CoRR},
  volume       = {abs/1804.04235},
  year         = {2018},
  url          = {http://arxiv.org/abs/1804.04235},
  eprinttype    = {arXiv},
  eprint       = {1804.04235},
  timestamp    = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1804-04235.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{sukhbaatar2015,
  author       = {Sainbayar Sukhbaatar and
                  Arthur Szlam and
                  Jason Weston and
                  Rob Fergus},
  title        = {Weakly Supervised Memory Networks},
  journal      = {CoRR},
  volume       = {abs/1503.08895},
  year         = {2015},
  url          = {http://arxiv.org/abs/1503.08895},
  eprinttype    = {arXiv},
  eprint       = {1503.08895},
  timestamp    = {Mon, 13 Aug 2018 16:49:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SukhbaatarSWF15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gru,
  author       = {KyungHyun Cho and
                  Bart van Merrienboer and
                  Dzmitry Bahdanau and
                  Yoshua Bengio},
  title        = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
  journal      = {CoRR},
  volume       = {abs/1409.1259},
  year         = {2014},
  url          = {http://arxiv.org/abs/1409.1259},
  eprinttype    = {arXiv},
  eprint       = {1409.1259},
  timestamp    = {Mon, 13 Aug 2018 16:47:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ChoMBB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{wolf2019hf,
  author       = {Thomas Wolf and
                  Lysandre Debut and
                  Victor Sanh and
                  Julien Chaumond and
                  Clement Delangue and
                  Anthony Moi and
                  Pierric Cistac and
                  Tim Rault and
                  R{\'{e}}mi Louf and
                  Morgan Funtowicz and
                  Jamie Brew},
  title        = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  journal      = {CoRR},
  volume       = {abs/1910.03771},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.03771},
  eprinttype    = {arXiv},
  eprint       = {1910.03771},
  timestamp    = {Tue, 02 Jun 2020 12:49:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-03771.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{trisedya-etal-2019-neural,
    title = "Neural Relation Extraction for Knowledge Base Enrichment",
    author = "Trisedya, Bayu Distiawan  and
      Weikum, Gerhard  and
      Qi, Jianzhong  and
      Zhang, Rui",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1023",
    doi = "10.18653/v1/P19-1023",
    pages = "229--240",
    abstract = "We study relation extraction for knowledge base (KB) enrichment. Specifically, we aim to extract entities and their relationships from sentences in the form of triples and map the elements of the extracted triples to an existing KB in an end-to-end manner. Previous studies focus on the extraction itself and rely on Named Entity Disambiguation (NED) to map triples into the KB space. This way, NED errors may cause extraction errors that affect the overall precision and recall.To address this problem, we propose an end-to-end relation extraction model for KB enrichment based on a neural encoder-decoder model. We collect high-quality training data by distant supervision with co-reference resolution and paraphrase detection. We propose an n-gram based attention model that captures multi-word entity names in a sentence. Our model employs jointly learned word and entity embeddings to support named entity disambiguation. Finally, our model uses a modified beam search and a triple classifier to help generate high-quality triples. Our model outperforms state-of-the-art baselines by 15.51{\%} and 8.38{\%} in terms of F1 score on two real-world datasets.",
}

@inproceedings{genie,
    title = "{G}en{IE}: Generative Information Extraction",
    author = "Josifoski, Martin  and
      De Cao, Nicola  and
      Peyrard, Maxime  and
      Petroni, Fabio  and
      West, Robert",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.342",
    doi = "10.18653/v1/2022.naacl-main.342",
    pages = "4626--4643",
    abstract = "Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.",
}

@inproceedings{bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@inproceedings{seq2seq,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}
