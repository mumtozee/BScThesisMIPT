\section{Эксперименты}

В данном разделе описаны детали экспериментов проведенных используя вышеописанную методологию и метрики качества вместе с базовыми моделями. Тут важно отметить, что в экспериментах были использованы многоязычные версии популярных трансформеров, чтобы их можно было дообучить с небольшим количеством данных на целевом языке и получить рабочие модели на этом языке.

\subsection{Детали обучения}
Все модели обучались на двух видеокартах NVIDIA A100 40gb. В качестве оптимизатора был использован AdamW \cite{adamw} c \texttt{weight\textunderscore decay=0.01}. Шаги обучения и размеры batch'a варьировались в зависимости от модели. Ниже описаны детали по каждому подходу (размеры batch'a указаны для каждого вычислительного устройства):

\textbf{GenAE}   В качестве sequence-to-sequence моделей были выбраны mT5-small и mT5-base \cite{mt5}. Шаг обучения для обеих моделей был равен 0.0003, а размер batch'a для базового варианта модели 32, а для маленького варианта - 64. Обучение длилось 3 эпохи с классом \texttt{Seq2SeqTrainer} библиотеки Huggingface Transformers \cite{wolf2019hf}. Генерация происходит жадным декодированием, то есть в качестве следующего токена всегда выбирается токен с наибольшей условной вероятностью.

\textbf{Классификатор предикатов}   В качестве базовой архитектуры BERT-подобной модели для текстовой классификации были выбраны многоязычный DistilledBERT \cite{distilbert} и mBERT-base. Для multilabel classification были подобраны веса положительных классов как описано в формуле \ref{bce_loss_component} в одном эксперименте, и в другом - приравнивались 1. Шаг обучения для всех экспериментов с этой моделью был равен 0.00003, а для binary ranking и contrastive learning 0.00002. Размер batch'а для multilabel classification был равен 512, для contrastive learning - 10, для binary ranking - 128. Для binary ranking были так же рассмотрены mBERT-base и mDeBERTa-NLI base. NLI модель рассматривалась для улучшения качества определения релевантности предиката. Изначально, в экспериментах предикаты подавались на вход моделям как есть, позже было решено использовать их описания на естественном языке, что везде улучшило качество. Во всех подходах модели обучались в течение 5 эпох, кроме contrastive learning в котором обучение длилось 4 эпохи.

Процессы получения предсказаний у подходов binary ranking и contrastive learning очень похожи, то есть обе модели принимают на вход пару реплики и описания предиката и дают на выходе число, либо вероятность релевантности предиката, либо схожесть описания предиката и реплики, и для корректности предсказаний требуют установления определенного порога. Порог можно подобрать жадным образом. Для простоты, в данной работе порог был подобран для всех предикатов один в обоих подходах. Это также связано с предположением, что в данных подходах модели не совсем ограничены предопределенным множеством предикатов и обучаются на описаниях отношений на естественном языке, а не на самих предикатах, для достижения обобщающей способности. Таким образом:

\begin{enumerate}
    \item для binary ranking выбирается равномерная сетка в отрезке $[0, 1]$ и считается multi-label micro-averaged F1 на валидационной выборке и выбирается порог с наибольшим показателем данной метрики;
    \item для constrastive learning действуем аналогично пункту 1, но равномерная сетка строится на отрезке $[\min, \max]$, где $\min$ - минимальное значение схожести на валидационной выборке, а $\max$ - соответственно наибольшее;
    \item если же в contrastive learning модель на выходе дает числа из специфичного диапазона, например из $[-1, 1]$ или $[0, 1]$ поступаем аналогично пункту 1.
\end{enumerate}

\textbf{Генератор сущностей}    Для этого подхода был использован mT5-small. В качестве оптимизатора был изначально выбран AdaFactor \cite{adafactor}, с помощью которого все Т5 модели были предобучены, однако независимо от размера шага, с этим оптимизатором генератор сущностей переобучался начиная уже с ранних этапов обучения. Из-за этого по итогу было решено обучать только оптимизатором AdamW с \texttt{lr=0.00005} с размером batch'a 64. Дополнительно, для ускорения обучения модель была обучена в mixed-precision режиме с float16, но по итогу эксперимента во всех случаях было переполнение в представлении вещественных чисел float16. В каждом подходе модели обучались в течение 5 эпох. Генерация как и в GenAE, происходит жадным декодированием.

\textbf{Метрики качества}   Чтобы оценить итоговое качество извлечения триплетов из реплик берутся accuracy, F1 и BLEU-1. Accuracy и F1 считаются по точному совпадению полученных триплетов: recall равен отношению количества правильно сгенерированных триплетов на количество всех триплетов в тестовой выборке, а precision - это отношение количества правильно сгенерированных триплетов к количеству всех сгенерированных триплетов. В то время, как BLEU-1 более гибкий и учитывает нестрогое совпадение между триплетами. 
Для оценки качества классификатора предикатов применялись accuracy и multi-label F1. А генератор сущности оценивался в конечном режиме, когда в качестве извлеченных предикатов подавались правильные отношения из датасета.

\textbf{GenAE на WikiNRE}   Для оценки качества sequence-to-sequence подхода был проведен дополнительный эксперимент на датасете WikiNRE \cite{trisedya-etal-2019-neural}. Детали эксперимента и результаты описаны в Приложении \ref{AppendixA}.

