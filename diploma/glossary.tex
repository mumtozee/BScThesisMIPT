\chapter{\centering{\normalsize{ОПРЕДЕЛЕНИЯ, ОБОЗНАЧЕНИЯ И СОКРАЩЕНИЯ}}}
\addcontentsline{toc}{section}{ОПРЕДЕЛЕНИЯ, ОБОЗНАЧЕНИЯ И СОКРАЩЕНИЯ}

В настоящем отчете о НИР применяют следующие сокращения и обозначения:

Авторегрессионная генеративная модель --- модель генерирующая следующий элемент входной последовательности, неявно используя условное распределение следующего элемента по предыдущим.

Метод k-ближайших соседей (англ.: k-nearest neighbors algorithm, kNN) --- алгоритм для автоматической классификации объектов или регрессии.

Персона --- описание личности в разговорных диалоговых датасетах. Обычно состоит из 5-6 коротких предложений о пользователе и его предпочтениях.

Трансформер (англ.: Transformer) --- архитектура глубоких нейронных сетей, использующая механизм внимания для повышения скорости обучения.

Триплет --- кортеж из трех элементов: (субъект, отношение, объект). Отношение или предикат определяется из конечного множества.

Batch --- подмножество объектов из всего датасета, которое обычно выбирается случайно. Используется в обучении современных больших нейронных сетей с помощью стохастического градиентного спуска или Adam.

BERT (сокращ.: Bidirectional Encoder Representations from Transformers, рус.: двунаправленные векторные представления трансформера) --- архитектура и метод предобучения трансформера, при котором модель должна предсказать токены последовательности, замененные на специальный MASK-токен, а также, является ли второе предложение из двух поданных на вход продолжением первого в тексте.

Checkpoint --- промежуточные сохраненные веса модели во время обучения для фиксации состояния обучения.

Сollaborative filtering --- подход к построению рекомендательных систем, при котором контент/товары рекомендуются на основании схожести пользователей. 

Сontent-based filtering --- подход к построению рекомендательных систем, при котором пользователи сопоставляются с контентом или товарами, которые им нравятся.

Distant supervision --- прием использующийся в построении графов знаний. Основная идея и предпорложение приема заключается в том, что если какие то две сущности находятся в определенном отношении, то любое предложение содержащее эти сущности выражает это отношение.

Encoder-decoder --- архитектура моделей которые решают задачи sequence-to-sequence. Encoder получает некоторые представление входных данных, а decoder преобразовывает это представление в выходные данные.

Few-shot --- разновидность обучения с учителем, когда обучающих примеров чрезвычайно мало: обычно 5 или 1.

Framework --- комплексное программное обеспечение которое содержит в себе методы преобразования данных и обучение моделей вместе с методами оценки их качества.

Gated Recurrent Unit (сокращ.: GRU, рус.: управляемый рекуррентный блок) --- разновидность архитектуры рекуррентных нейронных сетей, обладающая улучшенными свойствами обработки долговременных последовательностей, но при этом имеющая меньшее число параметров по сравнению с LSTM.

GBDT (Gradient Boosting Decision Tree) --- алгоритм дерева решений, основанный на итеративном накоплении.

Logit --- выходные данные модели до применения активации Sigmoid или Softmax.

LSTM (Long Short-term Memory, рус.: долгая краткосрочная память) --- разновидность архитектуры рекуррентных нейронных сетей, обладающая улучшенными свойствами обработки долговременных последовательностей.

MLM (сокращ.: masked language modelling) --- задача в обработке естественного языка, когда маскируется случайное слово во входном предложении и модель должна предсказать какое слово было замаскировано. Используется в предобучении BERT-подобных моделей.

Multi-Head Self-Attention --- разновидность механизма внимания, которая дает возможность каждому входному вектору взаимодействовать с другими входными векторами.

Multi-label classification --- разновидность задач обучения с учителем, когда по входному объекту предсказывается сразу некоторое множество классов, которое может быть и пустым. Например теги к фильмам в онлайн кинотеатре.

Point-wise Feed-Forward Network --- разновидность сети прямого распространения, состоящая из двух слоев и решающая задачу регрессии.

Position-wise Feed-Forward Network --- разновидность сети прямого распространения, состоящая из двух полносвязных слоев, применяемых к последнему измерению, что означает, что для каждого элемента последовательности используются одни и те же полносвязные слои.

Retrieval (рус.: ретрив) --- процесс извлечения наиболее релевантных к входным данным элементов из данного набора кандидатов, например ответа на вопрос.

Seed --- число или вектор использующийся для инициализации псевдослучайного генератора чисел.

Self-Attention --- разновидность механизма внимания, выявляющая закономерности только между входными данными.

Sequence-to-sequence --- генерация нейронной сетью выходной последовательности векторов по входной последовательности, при этом каждый следующий токен выходной последовательности генерируется с использованием предыдущих сгенерированных токенов.

Seq2seq --- Sequence-to-sequence модель глубокого обучения, принимающая на вход последовательность элементов, и возвращающая другую последовательность элементов.

SOTA (сокращ.: state-of-the-art) --- высший уровень развития некоторой технологии, или метод который получил самый лучший результат в определенной задаче.
